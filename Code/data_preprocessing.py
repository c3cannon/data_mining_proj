# -*- coding: utf-8 -*-
"""Data Preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16aoXH78W9mdh6k2oD597NEIxJmRyiiLH
"""

# PLACE ALL IMPORTS HERE
import pandas as pd
import sys
from google.colab import drive
import sklearn
import sklearn.impute, sklearn.pipeline
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
import sklearn.metrics as metrics
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from sklearn.model_selection import GridSearchCV


from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score

from IPython.display import display
import matplotlib.pyplot as plt
import seaborn as sns

"""# Initialization"""

# MOUNT YOUR GOOGLE DRIVE 
# (It only mounts to your private Virtual Machine, it doesn't expose your drive to anyone else)
drive.mount("/content/drive", force_remount=False)

# LOAD THE FILE
df = pd.read_csv("/content/drive/My Drive/Data Mining Project - Share/Data/combined.csv")
# Converting the column to DateTime format
df.Date = pd.to_datetime(df.Date, format='%Y-%m-%d')
df = df.set_index('Date')
df.head()

# Convert to numerical values
for col in df.columns:
  if (col != "freq"):
    df[col] = pd.to_numeric(df[col], errors="coerce")

display(df.head())

"""# Exploratory Data Analysis"""

#display(df.info())
#pd.value_counts(df["freq"])
#display(df[df["freq"]=="."])
display(df)

print("First valid Spend ACF:",df["spend_acf"].first_valid_index())
print("First valid Test:", df["test_count"].first_valid_index())
print("First valid bg_posts:", df["bg_posts"].first_valid_index())

"""# Preprocessing

## Filtering
"""

# DO PREPROCESSING

# Drop January and November
df_simple = df[df["month"].gt(1) & df["month"].lt(11)].copy()
# Drop cumulative COVID data (better to use rates/new counts)
#df_simple.drop(["case_count", "death_count", "test_count"], axis=1, inplace=True)

#df_simple['date'] = pd.to_datetime(df_simple[['year','month','day']])

#df_simple.shape
df_simple.head()

"""## Imputing/Filling

Stategy: Don't use Simple Imputer, which takes the mean/median/mode/etc. across all records in a feature. This would lose the time sensitive data. Here we are using "method = time" for imputation. Feel free to change it to linear if you want or anything else and test it
"""

# First fill COVID data
covid_ids = ["death", "case", "test", "positives"]
# If March or earlier, fill NAs with zero
for col in df_simple.columns:
  for covid_id in covid_ids:
    if covid_id in col:
      df_simple.loc[df_simple["month"]<=3, col] = df_simple.loc[df_simple["month"]<=3, col].fillna(0, inplace=False)
      # Also force all to be >= 0
      df_simple.loc[:, col] = df_simple.loc[:, col].clip(lower=0, inplace=False)
      #df_simple.loc[col, df_simple[col].isna()] = 0


#df_simple = df_simple.drop(['year','month','day','freq'],axis=1)
## Impute the DATA
count = df_simple.isna().sum()
print(count)
df_simple = df_simple.interpolate(method = 'time')

#df_slinear = df_simple.assign(InterpolateSLinear=df_simple.target.interpolate(method='slinear'))
count_2 = df_simple.isna().sum()
print(count_2)

display(df_simple[df_simple.isna().any(axis=1)])

print(df_simple[df_simple.isna().any(axis=1)]["statefips"].value_counts().shape)
print(df_simple[df_simple.isna().any(axis=1)]["statefips"].value_counts())

## This is just a function used to get the covid delays, 
## just pass to it a delay vector and it can delay the covid data by any element in the vector
def df_derived_by_shift(df,lag=0,concat=True):
    if (concat):
      df_copy = df.copy()
    else:
      df_copy = pd.DataFrame(data=None, columns=None, index=df.index)

    if not lag:
        return df
    cols ={}
    for i in lag:
        for x in list(df.columns):
                if not x in cols:
                    cols[x] = ['{}_{}'.format(x, i)]
                else:
                    cols[x].append('{}_{}'.format(x, i))
    for k,v in cols.items():
        columns = v
        dfn = pd.DataFrame(data=None, columns=columns, index=df.index)    
        i = 0
        for c in columns:
            dfn[c] = df[k].shift(periods=lag[i]*51) ## this is because we want to shift all the 51 states
            i = i+1
        df_copy = pd.concat([df_copy, dfn], axis=1)
    return df_copy

covid_columns = ["case_rate", 	
                 "death_rate", 	
                 "test_rate", 	
                 "new_positives_rate",
                 "new_case_rate", 	
                 "new_death_rate", 	
                 "new_test_rate", 	
                 "new_case_count", 	
                 "new_death_count", 	
                 "new_test_count"]

states = ["statefips"]  

spending_columns = ["spend_acf",	"spend_aer",	"spend_all",	"spend_apg",	"spend_grf",	"spend_hcs","spend_tws",
                    "spend_all_inchigh",	"spend_all_inclow",	"spend_all_incmiddle"]

revenues_columns = ["revenue_all",	"revenue_ss40",	"revenue_ss60",	"revenue_ss65",
                    "revenue_ss70",	"merchants_all",	"merchants_ss40",	"merchants_ss60",	"merchants_ss65",	"merchants_ss70"]

employment_columns = ["emp_combined",	"emp_combined_inclow",	"emp_combined_incmiddle", "emp_combined_inchigh",	"emp_combined_ss40",	"emp_combined_ss60",
                      "emp_combined_ss65",	"emp_combined_ss70","initclaims_count_regular",	"initclaims_rate_regular",	"contclaims_count_regular",	
                      "contclaims_rate_regular",	"initclaims_count_pua",	"contclaims_count_pua",	"contclaims_count_peuc",	"initclaims_rate_pua",
                      "contclaims_rate_pua",	"contclaims_rate_peuc","initclaims_count_combined",	"contclaims_count_combined",	"initclaims_rate_combined",
                      "contclaims_rate_combined",	"bg_posts","bg_posts_ss30",	"bg_posts_ss55",	"bg_posts_ss60",	"bg_posts_ss65",	"bg_posts_ss70",
                      "bg_posts_jz1",	"bg_posts_jzgrp12","bg_posts_jz2","bg_posts_jz3",	"bg_posts_jzgrp345",	"bg_posts_jz4",	"bg_posts_jz5"]


# df_simple["days_since_2020Feb01"] = (df_simple["date"] - pd.to_datetime("2020-02-01")).dt.days
# df_simple = df_simple.set_index(["date", "statefips"])

# df_econ = df_simple.drop(["year", "month", "day", "freq"], axis=1).drop(covid_columns, axis=1)
# keep = ["statefips"]
# keep.extend(covid_columns)
# keep.append("days_since_2020Feb01")

df_covid = df_simple.filter(covid_columns, axis=1)
df_states = df_simple.filter(states, axis=1)
df_spending = df_simple.filter(spending_columns, axis=1)
df_revenues = df_simple.filter(revenues_columns, axis=1)
df_employment = df_simple.filter(employment_columns, axis=1)

display(df_states.head())
display(df_covid.head())

## This is where we use the previous function to delay the covid data which is already imputed

delay = [3,7,10]
## delay_spending = []
df_covid = df_derived_by_shift(df_covid, delay)
## df_spending = df_derived_by_shift(df_spending, delay_spending)
display(df_covid.head())

print(df_covid["new_case_count"][0+2])
print(df_covid["new_case_count_3"][51*3+2])
print(df_covid["new_case_count_7"][51*7+2])

print(df_covid["new_case_count"])

df_covid.to_csv('data.csv')
!cp data.csv "drive/My Drive/"

## As you can see now we have NANs that arises from 2 things:
## 1- The imputation method won't fill all of them
## 2- Shifting the data will introduce NAN entries
## Let's remove all of the NAN entries now. I am appending the data again because we need all the data to have the same index
frames = [df_states, df_covid, df_spending, df_revenues,df_employment]
df_new = pd.concat(frames,axis=1)
df_new = df_new.dropna()
display(df_new.head())

## Save all the preprocessed data in drive
non_covid = states + spending_columns +  revenues_columns + employment_columns

df_states = df_new.filter(states, axis=1)
df_spending = df_new.filter(spending_columns, axis=1)
df_revenues = df_new.filter(revenues_columns, axis=1)
df_employment = df_new.filter(employment_columns, axis=1)
df_covid = df_new.drop(non_covid, axis=1)


econ_frames = [df_spending, df_revenues,df_employment]
df_econ = pd.concat(econ_frames,axis=1)

display(df_covid.head())
display(df_econ.head())

df_econ_shift1 = df_derived_by_shift(df_econ, [1], False)
display(df_econ_shift1)

print(df_econ["spend_acf"][0])
print(df_econ_shift1["spend_acf_1"][51*1])
print(df_econ.shape)
print(df_econ_shift1.shape)

df_econ_shift7 = df_derived_by_shift(df_econ, [7], False)
print(df_econ_shift7.shape)

df_states.to_csv('df_states.csv')
!cp df_states.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_covid.to_csv('df_covid.csv')
!cp df_covid.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_spending.to_csv('df_spending.csv')
!cp df_spending.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_revenues.to_csv('df_revenues.csv')
!cp df_revenues.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_employment.to_csv('df_employment.csv')
!cp df_employment.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_econ.to_csv('df_econ.csv')
!cp df_econ.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_econ_shift1.to_csv('df_econ_shift1.csv')
!cp df_econ_shift1.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_econ_shift7.to_csv('df_econ_shift7.csv')
!cp df_econ_shift7.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

def regression_results(y_true, y_pred):
# Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) 
    mse=metrics.mean_squared_error(y_true, y_pred) 
    #mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)
    print('explained_variance: ', round(explained_variance,4))    
    #print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))

new_frames_covid = [df_states,df_covid]
new_frames_econ = [df_states,df_econ]

df_covid_new = pd.concat(new_frames_covid,axis=1)
df_econ_new = pd.concat(new_frames_econ,axis=1)

display(df_covid_new.head())
display(df_econ_new.head())

"""## Now running algorithms"""

## In this block I am using Random Forest Regressor to predict df_econ using df_covid data while doing target encoding
orig_stdout = sys.stdout
f = open('/content/drive/My Drive/Data Mining Project - Share/Data/.txt', 'w')
sys.stdout = f

for column in df_econ_new.columns:

      if column != 'statefips':

          ## This is the Target Encoding Function 
          # means = df_econ_new.groupby('statefips')[column].mean()
          # df_econ_encoded = df_econ_new.copy()
          # temp = df_econ_encoded['statefips'].map(means)
          # df_covid_encoded = df_covid_new.copy()
          # df_covid_encoded['statefips'] = temp

          #This is without target encoding and dropping the states
          

          ## Preprocess the data using StandardScaler
          X_train = df_covid_encoded[:'2020-09']
          X_test = df_covid_encoded['2020-10':]
          X_scaler = StandardScaler()
          X_train = X_scaler.fit_transform(X_train)
          X_test = X_scaler.transform(X_test)

          y_train = df_econ_encoded.loc[:'2020-09', column]
          y_test = df_econ_encoded.loc['2020-10':, column]

          ## Fit a Random Forest Regressor and Find the best Parameters
          model = RandomForestRegressor()
          param_search = { 
              'n_estimators': [20, 50, 100],
              'max_features': ['auto', 'sqrt', 'log2'],
              'max_depth' : [i for i in range(5,15)]
          }
          tscv = TimeSeriesSplit(n_splits=3)
          gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = 'r2')
          gsearch.fit(X_train, y_train)
          best_score = gsearch.best_score_
          best_model = gsearch.best_estimator_

          y_true = y_test.values
          y_pred = best_model.predict(X_test)

          print('Regression results Using Random-Forest-Regressor on', column, ' are:')
          regression_results(y_true, y_pred)

sys.stdout = orig_stdout
f.close()

## Preprocess the data using StandardScaler
df_covid_encoded = df_covid_new
df_covid_encoded = df_covid_encoded.drop(['statefips'],axis=1)

df_econ_encoded = df_econ_new
df_econ_encoded = df_econ_encoded.drop(['statefips'],axis=1)

X_train = df_covid_encoded[:'2020-09']
X_test = df_covid_encoded['2020-10':]
X_scaler = StandardScaler()
X_train = X_scaler.fit_transform(X_train)
X_test = X_scaler.transform(X_test)

y_train = df_econ_encoded.loc[:'2020-09', 'spend_all']
y_test = df_econ_encoded.loc['2020-10':, 'spend_all']

## Fit a Random Forest Regressor and Find the best Parameters
model = RandomForestRegressor()
param_search = { 
    # 'n_estimators': [20, 50, 100],
    'n_estimators': [20],
    # 'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5,10,15]
}
tscv = TimeSeriesSplit(n_splits=10)
gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = 'r2')

gsearch.fit(X_train, y_train)
best_score = gsearch.best_score_
best_model = gsearch.best_estimator_

y_true = y_test.values
y_pred = best_model.predict(X_test)

print('Regression results Using Random-Forest-Regressor on spend_all:')
regression_results(y_true, y_pred)

## This is an example on how to write results to a file

# y_1 = [1,1,1,2,3,4,5]
# y_2 = [2,3,5,5,7,8,9]

# orig_stdout = sys.stdout
# f = open('/content/drive/My Drive/Data Mining Project - Share/Data/Example2.txt', 'w')
# sys.stdout = f
# for i in range(2):
#     print ("These are the results for ", i)
#     regression_results(y_1, y_2)

# sys.stdout = orig_stdout
# f.close()

# # Spot Check Algorithms
# models = []
# models.append(('LR', LinearRegression()))
# models.append(('NN', MLPRegressor(solver = 'lbfgs')))  #neural network
# models.append(('KNN', KNeighborsRegressor())) 
# models.append(('RF', RandomForestRegressor(n_estimators = 10))) # Ensemble method - collection of many decision trees
# models.append(('SVR', SVR(gamma='auto'))) # kernel = linear
# # Evaluate each model in turn
# results = []
# names = []
# for name, model in models:
#     # TimeSeries Cross validation
#  tscv = TimeSeriesSplit(n_splits=25)
    
#  cv_results = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')
#  results.append(cv_results)
#  names.append(name)
#  print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
    
# # Compare Algorithms
# plt.boxplot(results, labels=names)
# plt.title('Algorithm Comparison')
# plt.show()

# VARIANT 1: KNN Imputation
def distance(X, Y, **kwargs):
  #d = sklearn.metrics.pairwise.nan_euclidean_distances(X.reshape(1, -1), Y.reshape(1, -1), squared=True)
  d = 1

  #  
  # POSSIBLE ITERATION/IMPROVEMENT: Use a distance-matrix to create the KNN distance metric
  #
  if (X[0] != Y[0]):
    # Weight state distance twice as high
    d *= 4
  # Add extra weight to time distance
  d += 2 * (X[-1] - Y[-1])**2 / 7**2

  # Same State, Same Time = 1
  # Same State, 1 Week = 3
  # Different State, Same Time = 4
  # Different State, 1 week = 6

  np.sqrt(d)
#imputer = sklearn.impute.KNNImputer(n_neighbors = 1, weights="uniform", metric=distance)

# VARIANT 2: Iterative Imputer
from sklearn.experimental import enable_iterative_imputer
imputer = sklearn.impute.IterativeImputer(initial_strategy="most_frequent", n_nearest_features=8)

#pipe = sklearn.pipeline.Pipeline()
#pipe.steps.append(("impute", imputer))
#pipe.fit(df_econ)

imputer.fit(df_econ.to_numpy())
display(df_econ.to_numpy())
econ_processed = imputer.transform(df_econ.to_numpy())

display(econ_processed)



"""## This is just a test Phase

"""

## This is just a function used to get for the covid delays
def df_derived_by_shift(df,lag=0,NON_DER=[]):
    df = df.copy()
    if not lag:
        return df
    cols ={}
    for i in range(1,lag+1):
        for x in list(df.columns):
            if x not in NON_DER:
                if not x in cols:
                    cols[x] = ['{}_{}'.format(x, i)]
                else:
                    cols[x].append('{}_{}'.format(x, i))
    for k,v in cols.items():
        columns = v
        dfn = pd.DataFrame(data=None, columns=columns, index=df.index)    
        i = 1
        for c in columns:
            dfn[c] = df[k].shift(periods=i)
            i+=1
        df = pd.concat([df, dfn], axis=1)
    return df

## Define a delay d here and we can look for the optimal value later through correlation
delay = 2
NON_DER = ['days_since_2020Feb01',]
df_covid_new = df_covid.drop('statefips',axis = 1)
df_covid_new = df_derived_by_shift(df_covid_new, delay, NON_DER)
df_covid_new = df_covid_new.dropna()

df_econ_new = df_econ[df_econ.shape[0] - df_covid_new.shape[0]:] ## adjust both to have the same length

## I know that it seems from the display that they don't have the same length. 
## This is because we need to impute first the covid data which contains 308 NAN values in new_positives_rate.
display(df_econ_new.head())
display(df_covid_new.head())



# Tommy
display(df_econ.head())
display(df_covid.head())


df_econ.to_pickle("/content/drive/My Drive/Data Mining Project - Share/Processed_Data/df_econ.pandas")
df_covid.to_pickle("/content/drive/My Drive/Data Mining Project - Share/Processed_Data/df_covid.pandas")

"""## Scaling/Finite Differencing"""



"""## Time-Shifting"""



"""# Scratch/Sandbox"""

# Drop January and November
df_scratch = df
# Drop cumulative COVID data (better to use rates/new counts)
df_scratch.drop(["case_count", "death_count", "test_count"], axis=1)

df_scratch['date'] = pd.to_datetime(df_scratch[['year','month','day']])

vars = ["case_rate", "death_rate", "test_rate", "new_positives_rate", "new_case_rate", "new_case_count", "new_test_count"]

for var in vars:
  plt.figure(figsize=(8,8))
  ax = plt.gca()
  for state in range(1,57):
    ax.set_xbound((pd.to_datetime("2020-01-01"), pd.to_datetime("2020-04-10")))
    plt.plot(df_scratch.loc[df_scratch["statefips"]==state, "date"], df_scratch.loc[df_scratch["statefips"]==state, var])
    plt.title(var)


#sns.lineplot(data=df_simple, x="date", y="case_rate", hue="statefips")

df_temp = df_covid.copy()
#df_temp = df_temp.set_index('date')
# TH: Updated index name
df_temp = df_temp.set_index(['days_since_2020Feb01', 'statefips'])
# df_temp = df_temp.resample('W').mean()

data_case_rate = df_temp["new_case_rate"]
data_death_rate = df_temp["new_death_rate"]
data_test_rate = df_temp["new_test_rate"]

df_temp = df_temp.drop(columns=["new_case_rate", "new_death_rate", "new_test_rate"])

display(df_temp.head())
print(df_temp.shape)
print(df_temp.columns.values)

#X_train = df_temp[:'2020-09-27']
#y_train = data_death_rate[:'2020-09-27']
#X_test = df_temp['2020-10-04':]
#y_test = data_death_rate['2020-10-04':]

# I changed the date index above which messed up this filtering... sorry. 
# I don't know how to work with Pandas very well, can't figure out how to filter like what you were going for above
X_train = df_temp[:120]
y_train = data_death_rate[:120]
X_test = df_temp[127:]
y_test = data_death_rate[127:]

models = []
models.append(('LR', LinearRegression()))
models.append(('NN', MLPRegressor(solver = 'lbfgs')))
models.append(('KNN', KNeighborsRegressor())) 
models.append(('RF', RandomForestRegressor(n_estimators = 10)))
models.append(('SVR', SVR(gamma='auto')))

# Evaluate each model in turn
results = []
names = []
for name, model in models:
  tscv = TimeSeriesSplit(n_splits=10)
  cv_results = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')
  results.append(cv_results)
  names.append(name)
  print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
    
# Compare Algorithms
plt.boxplot(results, labels=names)
plt.title('Algorithm Comparison')
plt.show()