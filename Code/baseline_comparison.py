# -*- coding: utf-8 -*-
"""Code- Hassan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8Y094uLGGs4rKHDBOAFn5NrXo416eIi
"""

# PLACE ALL IMPORTS HERE
import pandas as pd
import sys
from google.colab import drive
import sklearn
import sklearn.impute, sklearn.pipeline
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
import sklearn.metrics as metrics
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from sklearn.model_selection import GridSearchCV

import xgboost as xgb
from scipy.stats import uniform, randint


from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score

from IPython.display import display
import matplotlib.pyplot as plt
import seaborn as sns

"""# Initialization"""

# MOUNT YOUR GOOGLE DRIVE 
# (It only mounts to your private Virtual Machine, it doesn't expose your drive to anyone else)
drive.mount("/content/drive", force_remount=True)

# LOAD THE FILE
df = pd.read_csv("/content/drive/My Drive/Data Mining Project - Share/Data/combined.csv")
# Converting the column to DateTime format
df.Date = pd.to_datetime(df.Date, format='%Y-%m-%d')
df = df.set_index('Date')
df.head()

# Convert to numerical values
for col in df.columns:
  if (col != "freq"):
    df[col] = pd.to_numeric(df[col], errors="coerce")

display(df.head())

"""# Exploratory Data Analysis"""

#display(df.info())
#pd.value_counts(df["freq"])
#display(df[df["freq"]=="."])
display(df)

print("First valid Spend ACF:",df["spend_acf"].first_valid_index())
print("First valid Test:", df["test_count"].first_valid_index())
print("First valid bg_posts:", df["bg_posts"].first_valid_index())

"""# Preprocessing

## Filtering
"""

# DO PREPROCESSING

# Drop January and November
df_simple = df[df["month"].gt(1) & df["month"].lt(11)].copy()
# Drop cumulative COVID data (better to use rates/new counts)
#df_simple.drop(["case_count", "death_count", "test_count"], axis=1, inplace=True)

#df_simple['date'] = pd.to_datetime(df_simple[['year','month','day']])

#df_simple.shape
df_simple.head()

"""## Imputing/Filling

Stategy: Don't use Simple Imputer, which takes the mean/median/mode/etc. across all records in a feature. This would lose the time sensitive data. Here we are using "method = time" for imputation. Feel free to change it to linear if you want or anything else and test it
"""

# First fill COVID data
covid_ids = ["death", "case", "test", "positives"]
# If March or earlier, fill NAs with zero
for col in df_simple.columns:
  for covid_id in covid_ids:
    if covid_id in col:
      df_simple.loc[df_simple["month"]<=3, col] = df_simple.loc[df_simple["month"]<=3, col].fillna(0, inplace=False)
      # Also force all to be >= 0
      df_simple.loc[:, col] = df_simple.loc[:, col].clip(lower=0, inplace=False)
      #df_simple.loc[col, df_simple[col].isna()] = 0


#df_simple = df_simple.drop(['year','month','day','freq'],axis=1)
## Impute the DATA
count = df_simple.isna().sum()
print(count)
df_simple = df_simple.interpolate(method = 'time')

#df_slinear = df_simple.assign(InterpolateSLinear=df_simple.target.interpolate(method='slinear'))
count_2 = df_simple.isna().sum()
print(count_2)

## This is just a function used to get the covid delays, 
## just pass to it a delay vector and it can delay the covid data by any element in the vector
def df_derived_by_shift(df,lag=0):
    df = df.copy()
    if not lag:
        return df
    cols ={}
    for i in lag:
        for x in list(df.columns):
                if not x in cols:
                    cols[x] = ['{}_{}'.format(x, i)]
                else:
                    cols[x].append('{}_{}'.format(x, i))
    for k,v in cols.items():
        columns = v
        dfn = pd.DataFrame(data=None, columns=columns, index=df.index)    
        i = 0
        for c in columns:
            dfn[c] = df[k].shift(periods=lag[i]*51) ## this is because we want to shift all the 51 states
            i = i+1
        df = pd.concat([df, dfn], axis=1)
    return df

covid_columns = ["case_rate", 	
                 "death_rate", 	
                 "test_rate", 	
                 "new_positives_rate",
                 "new_case_rate", 	
                 "new_death_rate", 	
                 "new_test_rate", 	
                 "new_case_count", 	
                 "new_death_count", 	
                 "new_test_count"]

states = ["statefips"]  

spending_columns = ["spend_acf",	"spend_aer",	"spend_all",	"spend_apg",	"spend_grf",	"spend_hcs","spend_tws",
                    "spend_all_inchigh",	"spend_all_inclow",	"spend_all_incmiddle"]

revenues_columns = ["revenue_all",	"revenue_ss40",	"revenue_ss60",	"revenue_ss65",
                    "revenue_ss70",	"merchants_all",	"merchants_ss40",	"merchants_ss60",	"merchants_ss65",	"merchants_ss70"]

employment_columns = ["emp_combined",	"emp_combined_inclow",	"emp_combined_incmiddle", "emp_combined_inchigh",	"emp_combined_ss40",	"emp_combined_ss60",
                      "emp_combined_ss65",	"emp_combined_ss70","initclaims_count_regular",	"initclaims_rate_regular",	"contclaims_count_regular",	
                      "contclaims_rate_regular",	"initclaims_count_pua",	"contclaims_count_pua",	"contclaims_count_peuc",	"initclaims_rate_pua",
                      "contclaims_rate_pua",	"contclaims_rate_peuc","initclaims_count_combined",	"contclaims_count_combined",	"initclaims_rate_combined",
                      "contclaims_rate_combined",	"bg_posts","bg_posts_ss30",	"bg_posts_ss55",	"bg_posts_ss60",	"bg_posts_ss65",	"bg_posts_ss70",
                      "bg_posts_jz1",	"bg_posts_jzgrp12","bg_posts_jz2","bg_posts_jz3",	"bg_posts_jzgrp345",	"bg_posts_jz4",	"bg_posts_jz5"]


# df_simple["days_since_2020Feb01"] = (df_simple["date"] - pd.to_datetime("2020-02-01")).dt.days
# df_simple = df_simple.set_index(["date", "statefips"])

# df_econ = df_simple.drop(["year", "month", "day", "freq"], axis=1).drop(covid_columns, axis=1)
# keep = ["statefips"]
# keep.extend(covid_columns)
# keep.append("days_since_2020Feb01")

df_covid = df_simple.filter(covid_columns, axis=1)
df_states = df_simple.filter(states, axis=1)
df_spending = df_simple.filter(spending_columns, axis=1)
df_revenues = df_simple.filter(revenues_columns, axis=1)
df_employment = df_simple.filter(employment_columns, axis=1)

econ_frames = [df_spending, df_revenues,df_employment]
df_econ = pd.concat(econ_frames,axis=1)

display(df_econ.head())
display(df_covid.head())

## This is where we use the previous function to delay the covid data which is already imputed

delay_covid = [7,14,21]
delay_econ = [7,14,21]

df_covid = df_derived_by_shift(df_covid, delay_covid)
df_econ = df_derived_by_shift(df_econ, delay_econ)
## df_spending = df_derived_by_shift(df_spending, delay_spending)
display(df_covid.head())

df_covid.to_csv('data.csv')
!cp data.csv "drive/My Drive/"

## As you can see now we have NANs that arises from 2 things:
## 1- The imputation method won't fill all of them
## 2- Shifting the data will introduce NAN entries
## Let's remove all of the NAN entries now. I am appending the data again because we need all the data to have the same index
frames = [df_states, df_covid, df_econ]
df_new = pd.concat(frames,axis=1)
df_new = df_new.dropna()
display(df_new.head())

## Save all the preprocessed data in drive
non_covid = states + spending_columns +  revenues_columns + employment_columns

df_states = df_new.filter(states, axis=1)
df_spending = df_new.filter(spending_columns, axis=1)
df_revenues = df_new.filter(revenues_columns, axis=1)
df_employment = df_new.filter(employment_columns, axis=1)

df_covid = df_new.drop(non_covid, axis=1)


econ_frames = [df_spending, df_revenues,df_employment]
df_econ = pd.concat(econ_frames,axis=1)

display(df_covid.head())
display(df_econ.head())

df_states.to_csv('df_states.csv')
!cp df_states.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_covid.to_csv('df_covid.csv')
!cp df_covid.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_spending.to_csv('df_spending.csv')
!cp df_spending.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_revenues.to_csv('df_revenues.csv')
!cp df_revenues.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_employment.to_csv('df_employment.csv')
!cp df_employment.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

df_econ.to_csv('df_econ.csv')
!cp df_econ.csv "/content/drive/My Drive/Data Mining Project - Share/Data"

def regression_results(y_true, y_pred):
# Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) 
    mse=metrics.mean_squared_error(y_true, y_pred) 
    #mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)
    print('explained_variance: ', round(explained_variance,4))    
    #print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))

new_frames_covid = [df_states,df_covid]
new_frames_econ = [df_states,df_econ]

df_covid_new = pd.concat(new_frames_covid,axis=1)
df_econ_new = pd.concat(new_frames_econ,axis=1)

display(df_covid_new.head())
display(df_econ_new.head())

## In this block I am using Random Forest Regressor to predict df_econ using df_covid data while doing target encoding
orig_stdout = sys.stdout
f = open('/content/drive/My Drive/Data Mining Project - Share/Data/RF-Results-Covid.txt', 'w')
sys.stdout = f

for column in df_econ_new.columns:

      if column != 'statefips':

          ## This is the Target Encoding Function 
          means = df_econ_new.groupby('statefips')[column].mean()
          df_econ_encoded = df_econ_new.copy()
          temp = df_econ_encoded['statefips'].map(means)
          df_covid_encoded = df_covid_new.copy()
          df_covid_encoded['statefips'] = temp

          ## Preprocess the data using StandardScaler
          X_train = df_covid_encoded[:'2020-07']
          X_test = df_covid_encoded['2020-08-01':'2020-08-31']
          X_scaler = StandardScaler()
          X_train = X_scaler.fit_transform(X_train)
          X_test = X_scaler.transform(X_test)

          y_train = df_econ_encoded.loc[:'2020-07', column]
          y_test = df_econ_encoded.loc['2020-8-01':'2020-08-31':, column]

          ## Fit a Random Forest Regressor and Find the best Parameters
          # model = KNeighborsRegressor()
          model = RandomForestRegressor()
          # model = SVR()
          param_search = { 
               'n_estimators': [20],
              # 'max_features': ['auto', 'sqrt', 'log2'],
               'max_depth' : [5,10,15]

              # 'n_neighbors':[5,10,15,20,25]

            # 'kernel': ['rbf'], 
            # 'C': [1,10, 100,1000, 10000]
          }
          tscv = TimeSeriesSplit(n_splits=5)
          gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = 'r2')
          gsearch.fit(X_train, y_train)
          best_score = gsearch.best_score_
          best_model = gsearch.best_estimator_

          y_true = y_test.values
          y_pred = best_model.predict(X_test)

          print('Regression results Using RF on', column, ' using covid data are :')
          regression_results(y_true, y_pred)

sys.stdout = orig_stdout
f.close()

## This is the Target Encoding Function 
means = df_econ_new.groupby('statefips')['emp_combined'].mean()
df_econ_encoded = df_econ_new.copy()
temp = df_econ_encoded['statefips'].map(means)
df_covid_encoded = df_covid_new.copy()
df_covid_encoded['statefips'] = temp

## Preprocess the data using StandardScaler
X_train = df_covid_encoded[:'2020-07']
X_test = df_covid_encoded['2020-08-01':'2020-08-31']
X_scaler = StandardScaler()
X_train = X_scaler.fit_transform(X_train)
X_test = X_scaler.transform(X_test)

y_train = df_econ_encoded.loc[:'2020-07', 'emp_combined']
y_test = df_econ_encoded.loc['2020-8-01':'2020-08-31':, 'emp_combined']

## Fit a Random Forest Regressor and Find the best Parameters
# model = KNeighborsRegressor()
# model = RandomForestRegressor()
model = SVR()
param_search = {
   'kernel': ['sigmoid'], 
  # # 'gamma': [1e-4, 0.01, 0.1, 0.5, 0.9],
   'C': [1,10, 100,1000, 10000]
}
tscv = TimeSeriesSplit(n_splits=5)
gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = 'r2')
gsearch.fit(X_train, y_train)
best_score = gsearch.best_score_
best_model = gsearch.best_estimator_

y_true = y_test.values
y_pred = best_model.predict(X_test)

print('Regression results Using SVR on', 'emp_combined', ' are:')
regression_results(y_true, y_pred)

## In this block use XGBoost Regressor to predict df_econ using df_covid data  while doing target encoding
orig_stdout = sys.stdout
f = open('/content/drive/My Drive/Data Mining Project - Share/Data/XGB-Results-Covid-Econ.txt', 'w')
sys.stdout = f

for column in df_econ_new.columns:
      if column != 'statefips':

            means = df_econ_new.groupby('statefips')[column].mean()
            df_econ_encoded = df_econ_new.copy()
            temp = df_econ_encoded['statefips'].map(means)
            df_covid_encoded = df_covid_new.copy()
            df_covid_encoded['statefips'] = temp

            ## Preprocess the data using StandardScaler
            X_train = df_covid_encoded[:'2020-07']
            X_test = df_covid_encoded['2020-08-01':'2020-08-31']
            X_scaler = StandardScaler()
            X_train = X_scaler.fit_transform(X_train)
            X_test = X_scaler.transform(X_test)

            y_train = df_econ_encoded.loc[:'2020-07', column]
            y_test = df_econ_encoded.loc['2020-8-01':'2020-08-31':, column]


            model = xgb.XGBRegressor(objective="reg:squarederror", random_state=42)
            params = {
                "colsample_bytree": uniform(0.7, 0.3),
                "gamma": uniform(0, 0.5),
                "learning_rate": uniform(0.03, 0.3), # default 0.1 
                "max_depth": randint(2, 6), # default 3
                "n_estimators": randint(100, 150), # default 100
                "subsample": uniform(0.6, 0.4)
            }

            tscv = TimeSeriesSplit(n_splits=5)
            gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = 'r2')


            gsearch.fit(X_train, y_train)
            best_score = gsearch.best_score_
            best_model = gsearch.best_estimator_

            y_true = y_test.values
            y_pred = best_model.predict(X_test)

            print('Regression results Using XGB Regressor on', column, ' using both covid and econ data are:')
            regression_results(y_true, y_pred)

sys.stdout = orig_stdout
f.close()